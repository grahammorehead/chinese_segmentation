""" stacked_lstm.py

    Objects to manage simple BiLSTM layers  (A Bi-directional LSTM and the underlying LSTM cell it employs)

"""
import sys
import os, re
import time
import random
from numpy.random import choice
import numpy as np
import shutil
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from gmutils import err
import gmutils as gm
from gmutils import pytorch_utils as pu

from utils import *
from split_layer import SplitLayer

################################################################################

VECTORIZER_FILE  = "vectorizer.pkl"

################################################################################
# LAYERS

class SimpleBiLSTM(nn.Module):
    """
    Bi-directional LSTM, designed to read a whole line and make predictions on each interstitial.

    For a line containing N characters, N-1 predictions will be made.

    This layer also commands the "underlying" layers: left2right, right2left, splitLayer.
    This layer takes care of setting modes, and saving / loading.

    """
    def __init__(self, dim, options={}):
        """
        Instantiate the Layer (which will later be used to connect tensors)

        Parameters
        ----------
        vec_dim : int (width of the vector generated by the vectorizer)

        dim : int (width of vectors in the rest of the network)
        """
        super(SimpleBiLSTM, self).__init__()
        self.dim   = dim
        self.ttype = options.get('ttype')
        
        # Left to right LSTM
        self.left2rightA  = LSTMCell(dim, options=options)            # Outputs two length-dim vectors
        
        # Right to left LSTM
        self.right2leftA  = LSTMCell(dim, options=options)            # Outputs two length-dim vectors

        # Output decision layer
        self.splitLayer  = SplitLayer(dim, options=options)      # Outputs a one-dimensional length-2 vector
        
        self.training    = False   # By default, but set to True during training with 'train()'

        self.vectorizer = False
        if options.get('verbose'):   # Use for debugging
            self.vectorizer = gm.deserialize(VECTORIZER_FILE)


    def initialize(self):
        self.left2rightA.initialize()
        self.right2leftA.initialize()


    def get_zeros(self):
        """
        Get a zeros tensor with the right type and shape
        """
        return pu.var_zeros(self.dim, ttype=self.ttype)
        
        
    def get_parameters(self):
        """
        Return a list of trainable parameters for the PyTorch optimizer
        """
        parameters = list(self.parameters())
        parameters.extend( list( self.left2rightA.parameters() ))
        parameters.extend( list( self.right2leftA.parameters() ))
        parameters.extend( list( self.splitLayer.parameters() ))

        return parameters


    def training_mode(self):
        """
        Set models for training mode
        """
        self.train()
        self.left2rightA.train()
        self.right2leftA.train()
        self.splitLayer.train()


    def eval_mode(self):
        """
        Set models for evaluation mode, which has, for instance, no dropout
        """
        self.eval()
        self.left2rightA.eval()
        self.right2leftA.eval()
        self.splitLayer.eval()

        
    def save(self, dirpath):
        """
        Save the current state of the model
        """
        try:
            torch.save(self.state_dict(),  dirpath+'/StackedBiLSTM.pth')
        except:
            raise
        self.left2rightA.save(dirpath, '/left2rightA.pth')
        self.right2leftA.save(dirpath, '/right2leftA.pth')
        self.splitLayer.save(dirpath)


    def load(self, dirpath):
        """
        Load the models from a specified directory.  Toss exceptions because models won't exist on the first run.
        """
        try:
            state_dict = torch.load(dirpath+'/StackedBiLSTM.pth', map_location=lambda storage, loc: storage)
        except:
            pass
        self.left2rightA.load(dirpath, '/left2rightA.pth')
        self.right2leftA.load(dirpath, '/right2leftA.pth')
        self.splitLayer.load(dirpath)


    def forward(self, line):
        """
        The 'forward()' for this network.  This function takes one sample at a time, where each sample is a line of text.

        Each LSTM, as it proceeds character by character, it makes a prediction *after* having consumed the character on both sides of an interstice.

        Parameters
        ----------
        line : (tensor) list of N vectors (where each vector represents one character)

        Returns
        -------
        (tensor) list of N-1 pairs (each pair of floats represents one interstice-- both the probability of no split and yes split)
        """
        reverse = []
        inputs = []                 # List of list of tensor.  Will hold the cell and hidden states at each possible split location
        #                               i.e. the value associated with the first interstice will be at index 0, etc.
        
        LCA  = self.get_zeros()     # Initialize left2right cell state
        LHA  = self.get_zeros()     # Initialize left2right hidden state
        RCA  = self.get_zeros()
        RHA  = self.get_zeros()
        
        ###  LEFT-to-RIGHT PASS  ###
        """ Iterate over the characters in a line.  After each two characters AB are read, a prediction will be made about splitting A B
            
            char : number of characters read
            interstice : the interstice number associated with the next prediction

            For instance, in a line "ABC":
              char=0, interstice=-1 : Process first character (A), No prediction
              char=1, interstice=0  : Process second character (B), Make prediction (on AB)
              char=2, interstice=1  : Process third character (C), Make prediction (on BC)
        """
        if self.vectorizer:
            print("\n\nForward: ")
        for char, x in enumerate(line):
            if pu.has_improper_values(x):
                err([x])
                raise(NaN_Exeption)
            
            reverse = reverse + [x]
            # if self.vectorizer:
            #    actual = self.vectorizer.decode(x)
                
            interstice = char - 1   # First value of 'interstice' will be negative, and have no prediction

            if pu.has_improper_values(LHA):
                err([LHA])
                raise(NaN_Exeption)
            LCA, LHA = self.left2rightA(x, LCA, LHA)      # Run this character through the Left->-Right LSTM
            if pu.has_improper_values(LHA):
                err([LHA])
                raise(NaN_Exeption)

            if interstice >= 0:
                if self.vectorizer:
                    sys.stdout.write("(%d)"% interstice)
                inputs.append( [LCA, LHA] )               # For this interstice, start collecting tensors to be used for a prediction
                assert(inputs[interstice] == [LCA, LHA])  # Confirm that they're in the right place in the 'inputs' array

            if self.vectorizer:
                sys.stdout.write(actual)
                
        ###  RIGHT-to-LEFT PASS  ###
        """
            To ensure that we are always looking at the right interstice, we will:
              - ignore looking at the interstice until we have processed two characters
              - keep interstice as it was from the left-to-right pass
              - and decrement it after each prediction

            For instance, in a line "ABC":
              backchar=0, interstice=1 (because it was already 1) : Process third character (C), No prediction
              backchar=1, interstice=1 (because it was ignored in first iteration) : Process second character (B), Make prediction (on BC)
              backchar=2, interstice=0 : Process first character (A), Make prediction (on AB)
        """
        if self.vectorizer:
            print("\nBackward: ")
        for backchar, x in enumerate( reversed(line) ):   # Iterate backwards through the same list of vectors
            # if self.vectorizer:
            #    actual = self.vectorizer.decode(x)
            
            RCA, RHA = self.right2leftA(x, RCA, RHA)      # Run this character through the Right->-Left LSTM
            if pu.has_improper_values(RHA):
                err([RHA])
                raise(NaN_Exeption)
                
            if backchar >= 1:
                if self.vectorizer:
                    sys.stdout.write("(%d)"% interstice)
                inputs[interstice].extend([RCA, RHA])
                assert(len(inputs[interstice]) == 4)
                interstice -= 1
            
            if self.vectorizer:
                sys.stdout.write(actual)
                
        # Confirm that we have the right number of predictions:
        assert(len(line) == len(inputs)+1)
            
        ###  Combine output from both LSTMs and run through the SplitLayer  ###
        #   'preds' will have one fewer element than 'line' because each prediction applies to the interstitial between two elements of 'line'
        preds = None
        for item in inputs:
            LC, LH, RC, RH = item               # Just the output from the second layer of each stacking
            pred = self.splitLayer(LH, RH)      # Splitter layer takes as input the hidden state from both directions
            pred = torch.unsqueeze(pred, 0)     # Add a dimension so that 'tensor_cat()' will work correctly
            if preds is None:
                preds = pred
            else:
                preds = pu.tensor_cat(preds,  pred, 0)   # Build a stack of tensors
                
        return preds


    def generate_random_line(self, N):
        """
        Generate a line of N random vectors
        """
        def random_vector():
            vec = []
            for i in range(14):
                vec.append(2*random.random() -1)
            vec = pu.torchvar(vec, ttype=torch.DoubleTensor)
            return vec
        
        output = []
        while len(output) < N:
            output.append( random_vector() )
            
        return output        
                
                
#######################
class LSTMCell(nn.Module):
    """
    Basic LSTMCell

    Variable scheme
    ---------------
    X : the current input vector

    H : the hidden state

    U : matrices applied to input vectors

    W : matrices applied to the hidden state

    """
    def __init__(self, dim, options={}):
        """
        Instantiate the Layer (which will later be used to connect tensors)
        """
        super(LSTMCell, self).__init__()
        self.dim       = dim
        self.training  = False   # By default, but set to True during training with 'train()'
        self.activ = torch.nn.LeakyReLU(negative_slope=0.001, inplace=False)
        self.initialize()
        

    def initialize(self):
        self.Uf  = nn.Linear(self.dim, self.dim).double()
        self.Uc  = nn.Linear(self.dim, self.dim).double()
        self.Ui  = nn.Linear(self.dim, self.dim).double()
        self.Uo  = nn.Linear(self.dim, self.dim).double()
        self.Wf  = nn.Linear(self.dim, self.dim).double()
        self.Wc  = nn.Linear(self.dim, self.dim).double()
        self.Wi  = nn.Linear(self.dim, self.dim).double()
        self.Wo  = nn.Linear(self.dim, self.dim).double()
        
        
    def save(self, dirpath, name='LSTMCell'):
        """
        Save the model weights to disk
        """
        torch.save(self.state_dict(), dirpath+'/%s.pth'% name)
        
        
    def load(self, dirpath, name='LSTMCell'):
        """
        Load model weights from disk
        """
        try:
            state_dict = torch.load(dirpath+'/%s.pth'% name, map_location=lambda storage, loc: storage)
            self.load_state_dict(state_dict)
        except:
            pass

        
    def forward(self, X, C0, H0):
        """
        the 'forward()' for this network.  The mathematics of the gates will follow this scheme, where C0 and H0 are the values coming
        in from previous iterations of this cell and are used to compute H1 (the next hidden layer) and C1, C2 (cell state tensors both
        within this iteration -- sometimes called Ct~ and Ct).

        Parameters
        ----------
        X : Tensor, the current input vector

        C0 : Tensor, the previous cell state

        H0 : Tensor, the previous hidden state
        """
        if pu.has_improper_values(X):
            err([X])
            raise(NaN_Exeption)
        
        # Step 1 (Forget Gate): G = sigma(X * Uf + H0 * Wf)
        G = self.Uf(X) + self.Wf(H0)
        if pu.has_improper_values(G):
            err([G])
            raise(NaN_Exeption)
        G = torch.sigmoid(G)

        # Step 2 (for updating C): C1 = tanh(X * Uc + H0 * Wc)
        C1 = self.Uc(X) + self.Wc(H0)
        C1 = torch.tanh(C1)
        
        # Step 3 (for updating C): I = sigma(X * Ui + H0 * Wi)
        I = self.Ui(X) + self.Wi(H0)
        I = torch.sigmoid(I)
        
        # Step 4 (output gate): O = sigma(X * Uo + H * Wo)
        O = self.Uo(X) + self.Wo(H0)
        O = torch.sigmoid(O)
            
        # Step 5: C2 = G * C0 + I * C1
        C2 = G * C0 + I * C1
        
        # Step 6: H = O * tanh(C)
        H1 = O * torch.tanh(C2)
        
        return C2, H1

    
################################################################################
################################################################################
